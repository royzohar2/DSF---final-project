{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m KNeighborsRegressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF , ExpSineSquared , WhiteKernel ,Matern,Sum,Product\n",
    "from sklearn.model_selection import train_test_split ,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#receives a trained model\n",
    "\n",
    "def metrics_f(model , X_train ,X_test, y_train,y_test):\n",
    "    # Make predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Train MSE:\", train_mse)\n",
    "    print(\"Test MSE:\", test_mse)\n",
    "    print(\"Train RMSE:\", train_rmse)\n",
    "    print(\"Test RMSE:\", test_rmse)\n",
    "    print(\"Train MAE:\", train_mae)\n",
    "    print(\"Test MAE:\", test_mae)\n",
    "    print(\"Train R2:\", train_r2)\n",
    "    print(\"Test R2:\", test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#receives a trained model\n",
    "\n",
    "def residual_plot(model , X_train ,X_test, y_train,y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate mean squared error on the training and testing sets\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Print the mean squared error\n",
    "    print(\"Train MSE:\", train_mse)\n",
    "    print(\"Test MSE:\", test_mse)\n",
    "\n",
    "    # Create a residual plot\n",
    "    train_residuals = y_train_pred - y_train\n",
    "    test_residuals = y_test_pred - y_test\n",
    "    plt.scatter(y_train_pred, train_residuals, c='blue', marker='o', label='Training data')\n",
    "    plt.scatter(y_test_pred, test_residuals, c='green', marker='s', label='Testing data')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.hlines(y=0, xmin=0, xmax=50, lw=2, color='red')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(model , X_train ,X_test, y_train,y_test):    \n",
    "     y_train_pred = model.predict(X_train)    \n",
    "     y_test_pred = model.predict(X_test)       \n",
    "     # Create a residual plot     \n",
    "     train_residuals = y_train_pred - y_train    \n",
    "     test_residuals = y_test_pred - y_test     \n",
    "     plt.scatter(y_train_pred, train_residuals, c='blue', marker='o', label='Training data')     \n",
    "     plt.scatter(y_test_pred, test_residuals, c='green', marker='s', label='Testing data')     \n",
    "     plt.xlabel('Predicted values')     \n",
    "     plt.ylabel('Residuals')     \n",
    "     plt.legend(loc='upper left')     \n",
    "     plt.hlines(y=0, xmin=0, xmax=50, lw=2, color='red')     \n",
    "     plt.show()      \n",
    "     # Create a residual histogram using seaborn for the training set     \n",
    "      \n",
    "     sns.histplot(train_residuals, kde=True, color='blue', edgecolor='black')    \n",
    "     plt.axvline(x=np.mean(train_residuals), color='red', linestyle='--', label='Mean')    \n",
    "     plt.xlabel(\"Residuals\")     \n",
    "     plt.ylabel(\"Frequency\")     \n",
    "     plt.title(\"Residual Histogram (Training Set)\")     \n",
    "     plt.legend()     \n",
    "     plt.show()      \n",
    "     \n",
    "     # Create a residual histogram using seaborn for the test set     \n",
    "     sns.histplot(test_residuals, kde=True, color='green', edgecolor='black')    \n",
    "     plt.axvline(x=np.mean(test_residuals), color='red', linestyle='--', label='Mean')     \n",
    "     plt.xlabel(\"Residuals\")     \n",
    "     plt.ylabel(\"Frequency\")    \n",
    "     plt.title(\"Residual Histogram (Test Set)\")    \n",
    "     plt.legend()    \n",
    "     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#receives a trained model\n",
    "def feature_importances_LinearRegression(model,X_train):\n",
    "    # Print feature coefficients\n",
    "    for feature, coef in zip(X_train.columns, model.coef_):\n",
    "        print(\"{} coefficient: {:.3f}\".format(feature, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_hyperparameters(X_train, y_train):\n",
    "    # Define the kernels to test\n",
    "    kernels = [Sum(ConstantKernel(1.0, (1e-3, 1e3))*RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "                ExpSineSquared(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))),\n",
    "               Sum(ConstantKernel(1.0, (1e-3, 1e3))*RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "                WhiteKernel(noise_level=1e-2))]\n",
    "\n",
    "    # Define the hyperparameters for each kernel\n",
    "    hyperparameters = [{'kernel__k1__k1__constant_value': [0.1, 1, 10],\n",
    "                        'kernel__k1__k2__length_scale': [0.1, 1, 10],\n",
    "                        'kernel__k2__length_scale': [0.1, 1, 5],\n",
    "                        'kernel__k2__periodicity':[0.1, 1, 10],\n",
    "                        \"alpha\": [0.01, 0.1, 1.0]},\n",
    "                       {\"kernel__k1__k1__constant_value\": [1.0, 5.0, 10.0],\n",
    "                        \"kernel__k1__k2__length_scale\": [1.0, 5.0, 10.0],\n",
    "                        \"kernel__k2__noise_level\": [1e-4, 1e-3, 1e-2],\n",
    "                        \"alpha\": [0.01, 0.1, 1.0]}]\n",
    "\n",
    "    # Perform cross-validation to tune the hyperparameters\n",
    "    best_score = -np.inf\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        gp = GaussianProcessRegressor(kernel=kernel)\n",
    "        random_search  = RandomizedSearchCV(gp, hyperparameters[i],scoring='r2',n_iter=10 ,verbose=5, cv=5)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        if random_search.best_score_ > best_score:\n",
    "            best_score = random_search.best_score_\n",
    "            best_kernel = random_search.best_estimator_.kernel_\n",
    "\n",
    "    # Fit the Gaussian process to the data with the best kernel and hyperparameters\n",
    "    gp = GaussianProcessRegressor(kernel=best_kernel)\n",
    "    return gp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_hyperparameter(X_train, y_train):\n",
    "    n_list = list(range(3, 30, 2))\n",
    "    # Define parameter grid to search\n",
    "    param_grid = {'n_neighbors': n_list,\n",
    "                  'weights': ['uniform', 'distance'], # equal weight or weight to each point proportional to its inverse distance\n",
    "                  'p': [1, 2], #for minkowski :  Manhattan distance or  Euclidean distance \n",
    "                  'leaf_size': [10, 20, 30],#for faster nearest neighbor search.\n",
    "                  'metric': ['euclidean', 'manhattan', 'minkowski']}#compute the distance between two points in the dataset\n",
    "                 \n",
    "\n",
    "   \n",
    "    knn = KNeighborsRegressor()\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(knn, param_grid,scoring='r2', verbose=4, cv=5)\n",
    " \n",
    "    # fit gridsearchcv\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('Best Parameters:', grid_search.best_params_)\n",
    "    print('Best Score:', grid_search.best_score_)\n",
    "\n",
    "    # create knn_best using best parameters\n",
    "    knn_best = KNeighborsRegressor(**grid_search.best_params_)\n",
    "    return knn_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_hyperparameter(X_train, y_train):\n",
    "    hyperparameter = {\n",
    "    'n_estimators': [100,500,900,1100,1500],\n",
    "    'max_depth': [2,3,5,10,15],\n",
    "    'lerning_rate': [0.05,0.1,0.15,0.2],\n",
    "    'min_child_weight': [1,2,3,4],\n",
    "    'booster': ['gbtree','gblinear'],\n",
    "    'base_score': [0.025,0.5,0.75,1]    \n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor()\n",
    "\n",
    "    random_search  = RandomizedSearchCV(xgb , hyperparameter , cv =5 ,scoring = 'r2' ,verbose=4 ,n_iter = 150)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print best parameters and best score\n",
    "    print('Best Parameters:', random_search.best_params_)\n",
    "    print('Best Score:', random_search.best_score_)\n",
    "\n",
    "    # create xgb_best using best parameters\n",
    "    xgb_best = XGBRegressor(**random_search.best_params_)\n",
    "    return xgb_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one more model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
